{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7: Text classification with Keras\n",
    "\n",
    "The homework consists of only coding part (25 pts).\n",
    " - All theoretical questions must be answered in your own words, do not copy-paste text from the internet. Points can be deducted for terrible formatting or incomprehensible English.\n",
    " - Code must be commented. If you use code you found online, you have to add the link to the source you used. There is no penalty for using outside sources as long as you convince us you understand the code.\n",
    "\n",
    "**Note that coding part consists of two different notebooks.**\n",
    "\n",
    "*Once completed zip the entire directory containing this exercise and upload it to https://courses.cs.ut.ee/2020/nn/spring/Main/Practices.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you did this homework together with course mates, please write here their names (answers still have to be your own!).**\n",
    "\n",
    "**Name(s):** fill this in if applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this practice session we are looking into text classification. This means we are going to touch topics like word embeddings and recurrent neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SimpleRNN, LSTM, GRU\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.datasets import imdb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We are going to use IMDB moview review dataset for sentiment classification. Luckily, this is included with Keras again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_seq, y_train), (X_test_seq, y_test) = imdb.load_data()\n",
    "\n",
    "print('Train data shape: ', X_train_seq.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test_seq.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs are the movie reviews encoded as sequences of word indexes. There are couple of \"word indexes\" with special meaning: 1 means start of sentence, 2 means unknown word (word not in vocabulary) and 0 means padding (empty) and should be ignored. Index 3 just does not exist (imdb.load_data() weird speciality).\n",
    "\n",
    "All other numbers signify actualy words, with the smaller numbers coding for more frequently used words. Therefore 4 corresponds to the most used word in our database, 5 the second most used etc.\n",
    "\n",
    "You can see below one encoded review. As expected it starts with 1, also expectedly 4 and 5 look quite frequent words indeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model should learn to produce the sentiment of the review: 1 movie is good, 0 movie is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a data scientist you should always be a little bit more curious what the data is really about. In this case we might want to know what the text actually looked like. To decode the sequences of numbers into words, we need the number-to-word correspondences. Luckly Keras IMDB dataset has function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2num = imdb.get_word_index()\n",
    "word2num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this gives us only word-to-number encoding. We need to produce the inverse mapping (number-to-word) by ourselves.\n",
    "\n",
    "**Task 1 (2pts):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO: Construct a dictionary for the opposite mapping, from numbers to words.#\n",
    "################################################################################\n",
    "num2word = {} # fill this dict with the inverse mapping\n",
    "################################################################################\n",
    "#                             END OF YOUR CODE                                 #\n",
    "################################################################################\n",
    "num2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the most frequent words. Notice that this vocabulary indexing still starts from 1 (and not 4). Later when translating text into numbers or the other way round we will have to shift it by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "    print(str(i) + ':', num2word[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally see what those movie reviews were about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to decode reviews\n",
    "def nums2sentence(nums):\n",
    "    # the encoding has 0,1 and 2 as special numbers, 3 is omitted, therefore words start from nr 4\n",
    "    # word indexes start from 1, so to match the two sysems we have to shift by 3 \n",
    "    return \" \".join([\"UNK\" if i == 2 else num2word[i-3] for i in nums[1:]]) \n",
    "\n",
    "# print the first 3 reviews\n",
    "for i in range(3):\n",
    "    print(nums2sentence(X_train_seq[i]), \"- GOOD\" if y_train[i] == 1 else \"- BAD\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model\n",
    "\n",
    "Our goal in this exercise is to train a model to predict sentiment from the movie review text. Our neural networks expect fixed size inputs though, therefore we need to do some preprocessing first. In particular:\n",
    " - we need to make all sentences of the same length, so that the result can be represented as a matrix (tensor),\n",
    " - we might want to limit the vocabulary size to only certain number of more frequent words to save resources\n",
    "\n",
    "In the following you will write a function to do those preprocessing steps.\n",
    "\n",
    "**Task 2 (5pts):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_sequences(seq, vocab_size, maxlen):\n",
    "    out = []\n",
    "    ##############################################################################\n",
    "    # TODO: Write a function to preprocess sequences:                            #\n",
    "    #       - all sequences should have the same length - pad them with 0s at    #\n",
    "    #         the beginning (the extra zeros go in the beginning of the list)    #\n",
    "    #       - if sequence is too long (above maxlen), then keep only the maxlen  #\n",
    "    #         words in end of the review(works better than keeping the beginning)#\n",
    "    #       - remove all words with index >= vocab_size, replace them with 2s.   #\n",
    "    #         (Words are ordered by frequency, so you are in fact removing less  #\n",
    "    #         frequent words.)                                                   #\n",
    "    ##############################################################################\n",
    "    pass\n",
    "    ##############################################################################\n",
    "    #                             END OF YOUR CODE                               #\n",
    "    ##############################################################################\n",
    "    return np.array(out)\n",
    "\n",
    "vocab_size = 20000\n",
    "maxlen = 80\n",
    "\n",
    "X_train = preprocess_sequences(X_train_seq, vocab_size, maxlen)\n",
    "assert X_train.shape[1] == maxlen, \"Second dimension of training set must be equal to maximum sentence length.\"\n",
    "assert np.max(X_train) < vocab_size, \"The training set should not contain words with index >= vocab_size\"\n",
    "X_test = preprocess_sequences(X_test_seq, vocab_size, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a network. Unconventionally our input layer processes only integers this time, corresponding to the words. These are fed to Embedding layer where the are transformed into the word vectors (embeddings) that are fed to subsequent layers. Using `mask_zero=True` tells the network that 0 is a special value that denotes padding. Timesteps where input is 0 will not be considered when backpropagating the error signal (no learning happens at those timesteps). \n",
    "\n",
    "Output of Embedding layer goes to RNN layer that walks over its inputs while keeping the hidden state as memory. Normally RNN layers produce output at each timestep, but here we have directed it with `return_sequences=False` to produce the output only at the last timestep.\n",
    "\n",
    "As we have a binary classification task, we are using sigmoid as final activation and binary crossentropy as a loss function this time. If $y$ is the target value (1 or 0) and $p$ is the predicted probability (0..1) then the loss function looks like this:\n",
    "\n",
    "$$\n",
    "L = y \\log p + (1 - y) \\log (1 - p)\n",
    "$$\n",
    "\n",
    "This is basically equivalent to usual softmax categorical loss, where $y$ is the one-hot vector of target values and $p$ is the vector of probabilities that sums up to 1.\n",
    "\n",
    "$$\n",
    "L = \\sum_i y_i \\log p_i\n",
    "$$\n",
    "\n",
    "\n",
    "For more information see Keras documentation:\n",
    "\n",
    "* [Embedding layers](https://keras.io/layers/embeddings/)\n",
    "* [Recurrent layers](https://keras.io/layers/recurrent/)\n",
    "* [Loss functions](https://keras.io/losses/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definign a simple RNN \n",
    "embed_size = 128\n",
    "rnn_size = 128\n",
    "\n",
    "x = Input(shape=(None,), dtype='int32')\n",
    "e = Embedding(vocab_size, embed_size, mask_zero=True)(x)\n",
    "r = SimpleRNN(rnn_size, return_sequences=False)(e)\n",
    "p = Dense(1, activation='sigmoid')(r)\n",
    "\n",
    "rnn_model = Model(x, p)\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (2pts):** What are the three dimensions of embedding layer output? Why LSTM layer is missing one of them?\n",
    "\n",
    "**Your Answer:** *fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the simple RNN, should get accuracy above 50% (maybe even 60%)\n",
    "# you can change nr of epochs to train longer, if you can afford spending more time\n",
    "history = rnn_model.fit(X_train, y_train, batch_size=32, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RNN layer has limitations on how long it can keep its memory. The ability to remember can be imporved with gated recurrent layers like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit). Let's try to compare them! \n",
    "\n",
    "But for the comparison to be fair, the networks need to have approximately the same number of parameters. LSTM and GRU use additional gates that have additional weights. Your task is to create LSTM and GRU networks that have approximately the same number of parameters as the SimpleRNN network above and train them.\n",
    "\n",
    "**Task 4 (1.5pts):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: 1) Construct a network with LSTM layer that has approximately the    #\n",
    "#       same number of parameters as previous SimpleRNN network.             #                  \n",
    "#       2) compile the model and print out summary                           #\n",
    "#       3) train the model for one epoch (should get accuracy >80%)          #\n",
    "##############################################################################\n",
    "pass\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (1.5pts):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: 1) Construct a network with GRU layer that has approximately the     #\n",
    "#       same number of parameters as previous SimpleRNN network.             #                  \n",
    "#       2) compile the model and print out summary                           #\n",
    "#       3) train the model for one epoch (should get accuracy >80%)          #\n",
    "##############################################################################\n",
    "pass\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6 (1.5pt):** Which recurrent unit produced the best results? How much smaller the LSTM or GRU layer had to be to be comparable to SimpleRNN in number of parameters?\n",
    "\n",
    "**Your Answer:** *fill this in*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Assign the best model to model variable.                             #\n",
    "##############################################################################\n",
    "model = None\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out\n",
    "\n",
    "Let's try out the best model you have! Your job is to create your own movie reviews and make the model classify them as either good or bad.\n",
    "\n",
    "**Task 7 (3pts):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words2sequences(words):\n",
    "    return [word2num[w]+3 if w in word2num else 2 for w in text_to_word_sequence(words)]\n",
    "def sentences2sequences(sentences):\n",
    "    return [[1] + words2sequences(s) for s in sentences]\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Write one positive (> 0.9) and one negative (< 0.1) movie review.    #\n",
    "#       Try to write it yourself, do not just copy paste reviews from        #\n",
    "#       somewhere until you find one that works.                             #\n",
    "##############################################################################\n",
    "myreviews = [\n",
    "    \"\", # fill this with good review\n",
    "    \"\" # fill this with bad review\n",
    "]\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################\n",
    "\n",
    "myreviews_seq = sentences2sequences(myreviews)\n",
    "X_myreviews = preprocess_sequences(myreviews_seq, vocab_size, maxlen)\n",
    "model.predict(X_myreviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8 (1pt):** How would you grade your handmade artificial intelligence?\n",
    "\n",
    "**Your Answer:** *fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "As mentioned above the network receives inputs as one-hot vectors. Multiplying this one-hot vector with the first weight layer transforms the words into some kind of real-valued vectors, we say it preforms embedding. Notice that as there is only one 1 in the one-hot input vector, effectlively each word corresponds to one row in the weight matrix.\n",
    "\n",
    "We can think of the weights that correspond to each word as the representation of that word was given by the network. This representation depends on the task the network is performing, the dataset we use etc. The netowrk uses the representation that is most useful for it.\n",
    "\n",
    "In this section we want to see if there is some consistency or logic in the representations that the network gives to the words. Are some word more similarly represented than others? To do so we first need to compress the 128-dimensional representation (the nr of nodes in Embedding layer) into 2D, in odrer to plot it. We can achieve this by using Principal Component Analysis (read more about it if you don't know what it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract the weight layer and make sure it's size makes sense\n",
    "embed = model.layers[1].get_weights()[0]\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform Principal Component Analysis on the rows of this matrix. We want to extract only the two most important axis of variance, because we want to plot the points in that space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "embed_2d = PCA(n_components = 2).fit_transform(embed)\n",
    "embed_2d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at where different words are positioned in the space defined by the first two components of PCA.\n",
    "\n",
    "We define three sets of words that we color differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,8))\n",
    "word_idx=[]\n",
    "for word in [\"great\",\"awesome\",\"beautiful\",\"magnificent\",\"masterpiece\"]:\n",
    "    idx = words2sequences(word)[0]\n",
    "    word_idx.append(idx)\n",
    "    plt.scatter(embed_2d[idx,0],embed_2d[idx,1],color=\"red\",s=100)\n",
    "    plt.text(embed_2d[idx, 0], embed_2d[idx, 1], num2word[idx- 3],fontsize=20)\n",
    "for word in [\"bad\", \"terrible\", \"boring\", \"lame\"]:\n",
    "    idx = words2sequences(word)[0]\n",
    "    word_idx.append(idx)\n",
    "    plt.scatter(embed_2d[idx,0],embed_2d[idx,1],color=\"blue\",s=100)\n",
    "    plt.text(embed_2d[idx, 0], embed_2d[idx, 1], num2word[idx- 3],fontsize=20)\n",
    "\n",
    "for word in [\"actor\", \"producer\", \"director\", \"dog\",\"and\",\"the\"]:\n",
    "    idx = words2sequences(word)[0]\n",
    "    word_idx.append(idx)\n",
    "    plt.scatter(embed_2d[idx,0],embed_2d[idx,1],color=\"gray\",s=100)\n",
    "    plt.text(embed_2d[idx, 0], embed_2d[idx, 1], num2word[idx- 3],fontsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9 (1pt):** What meaning does the fist dimension of the PCA (the x-axis) seem to represent?\n",
    "\n",
    "**Your Answer:** *fill this in*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
